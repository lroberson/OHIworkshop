---
title: "SCCS_workshop_2019"
author: "Leslie Roberson"
date: "7/10/2019"
output: html_document
---

## Hadley's book
https://r4ds.had.co.nz/

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(maps)
library(maptools)
library(rgeos)
library(mapproj)
```

## instructions:
SCCS_R_workshop_2019.html
# extra model checking stuff from the full day workshop:
http://www.seascapemodels.org/data/data-wrangling-spatial-course.html

```{r load data}
dat_raw <- read_csv("/Users/leslieroberson/OneDrive - The University of Queensland/_data/_raw_data/SCCS_Workshop_2019/SCCS_data-for-course/copepods_raw.csv")

dat <- dat_raw

names(dat)
```

## reshape data

```{r reshape}
# go from wide to long
dat <- 
  gather(data = dat, 
         key = region, value = richness_raw, East, `Southern Ocean`, West, # make a region column and put the three regions as factor levels in that variable
         na.rm = TRUE) # had heaps of NAs where they didn't take any samples

head(dat)
```

## visual error checking
look at the spatial distribution of samples

```{r ggplot}
# This plot simply shows the location of every segment. You can kind of see the CPR surveys wrapping around the coast of Australia.
ggplot(data = dat, 
       mapping = aes(x = longitude, y = latitude)) + # I like being explicit with the data = and mapping =
  geom_point()
# You can read the above line as: Take dat, create an aes (aesthetic) where the x-axis is longitude and the y-axis is latitude, and finally add (+) a points layer (geom_point).

# We will add a group command to make sure lines from different silks aren’t connected
# bc the same silk sampling line had many samples at differnet lat long coords along the line
ggplot(data = dat, 
       aes(x = longitude, y = latitude, group = silk_id)) + # try without the group = silk_id to see big black blobs of samples -- it doesn't know which points to connect, connects them all left to right or something
  geom_line() # plot lines instead of points
```

## colour the silk IDs:

# Add a theme layer here to remove the legend. 
Bc there are so many silk values that the legend ends up WAY bigger than the plot itself. 
# wrap factor around silk_id in the colour command
so that silk IDs (which are numbers) would be treated as discrete color levels, rather than a continuous measure

```{r color factor}
ggplot(data = dat, 
       aes(x = longitude, y = latitude, group = silk_id, color = factor(silk_id))) +
  geom_line() +
  theme(legend.position = "none")
```

## Checks on richness
So far so good, now let’s look at the richness data, our main variable for analysis
plot locations again, but this time colour points by copepod species richness

```{r color factor}
ggplot(data = dat, 
       aes(x = longitude, y = latitude, color = richness_raw)) +
  geom_point()
```
Looks the same as before, but note the legend, which is now coloured by species richness. One of the smart things that  ggplot2 does is automatically scale axes based on the range of all the data we’ve plotted. This means our locations always fit nicely within the space of the map.

The colours are also an ‘axis’, note that ggplot has them going to all the way to about -1000. This is a bit odd, and suggests that there are richness values that are close to -1000 (though we might not be able to see them under the other points). Obviously we can’t have negative species richness values, that makes no sense.

Let’s try another plot of latitude versus richness to see if we can figure out what is going on

```{r look for errors}
ggplot(dat, aes(x = latitude, y = richness_raw)) +
  geom_point()
```
Ah, so most of the data are smallish (<100) positive values. But there are maybe three values near -1000

## Let’s use logical comparisons to learn more about these outliers

Logical comparisons evaluate to TRUE or FALSE. Here is a list of several useful operators for conducting logical comparisons in R:

< (less than) and <= (less than or equal to)

> (greater than) and >= (greater than or equal to)

== (equal to) and != (equal to)

& (AND) and | (OR) and ! (NOT)

```{r}
### This operation will return a TRUE or FALSE for every observation depending on if `richness_raw` is less than zero (TRUE) or not (FALSE) 
dat$richness_raw < 0 #prints out every result
```
Rather than print out every TRUE/FALSE individually, we can use table to count up the number of rows that have values less than (TRUE) or greater than (FALSE) zero

```{r table}
table(dat$richness_raw < 0)
```
We can fold dat_input$richness_raw < 0 back into a call to the data-frame to see what those rows are:

```{r filter}
filter(dat, richness_raw < 0 )
```
From this we can see that all negative values are -999. In some programs, this value indicates missing data. 

Turns out they should actually be richness_raw = 0.

So what we need to do now is change all the -999 to 0.

Perhaps the most familiar way for to do this is to use a spreadsheet editor to fix the -999. But that is slow and tedious with large datasets and is *not repeatable*.

What we want to do is use R to identify the mistakes, then correct them and create a new data frame. *So let’s use our logical indexing again:*

```{r indexing}
### replace negative values with 0
dat$richness_raw[dat$richness_raw < 0] <- 0
```
Let’s have a look at our graphs again:

```{r ggplot}
ggplot(data = dat, 
       aes(x = longitude, y = latitude, color = richness_raw)) +
  geom_point(size = 0.4)
```
Looks better (note the scale of the colour axis).

You might want to use logical indexing to double check all the values are really positive. This command will ask if any value is <0:

```{r filter}
dat %>% filter(richness_raw < 0) # nope, all good. 
```

At this point we might want to save the ‘corrected’ data frame as an external file, then only work with that one in the future:
```{r write csv}
write_csv(x = dat, path = "SSCS_data/copepods_corrected.csv")
```
Let’s do the latitude plot too. This is a really key plot for us, because we know that temperatures tend to get warmer at lower latitudes. So, if Prof Calanoid’s hypothesis is right, we would expect to see a decline in richness at higher latitudes:

```{r ggplot}
ggplot(data = dat, 
       aes(x = latitude, y = richness_raw)) +
  geom_point()
```
Something looks odd with this graph (and the map too), because we expected a strong gradient in richness with latitude. Well, at least we have some results.

Saving ggplots
Let’s save this figure and email it to Prof Calanoid to get their opinion:

```{r ggsave and geom_smooth}
### a convenient aspect of ggplot is that plots can be assigned to objects (cope_graph in this case) and used later...
cope_graph <- 
  ggplot(data = dat, aes(x = latitude, y = richness_raw)) +
  geom_point() +
  geom_smooth() # this basically fits a model without you haven't to do it yourself (with undertainty bars)

### ... like when it comes time to save the figure
ggsave(plot = cope_graph, filename = "figs/richness-latitude.png",
       width = 16, height = 10, units = "cm", dpi = 300)
```
ggsave is a function for saving plots generated by ggplot. Try ?ggsave to see all the options for changing the figure size, resolution, or file type.

## Join different datasets with dplyr
dplyr provides a useful set of functions for joining data frames by matching columns. Type ?inner_join in your console and you will get a list of all the join types dplyr supports.

Today we will use inner_join to join dat to the routes data using columns with the same names to match by. It will keep all rows from dat where there are matching rows in routes, so if rows don’t match, they will be removed (use left_join if you want to keep rows in dat that don’t match too). inner_join will also duplicate rows if there are multiple matches so after joining two dataframes always check that the join worked as expected!

# Turns out the copepod data was junk:
Prof Calanoid has now explained that we need to standardize richness estimates, because silks from different routes have different sizes.

Prof Calanoid had already provided the silk sizes in a file Route-data.csv, but neglected to tell us we needed to use this for a standardisation. No worries though, we can use our join skills to match the routes data and silk sizes to our richness data and then the standardization will be easy… right?

```{r import route data}
### import route data
routes_raw <- read_csv("/Users/leslieroberson/OneDrive - The University of Queensland/_data/_raw_data/SCCS_Workshop_2019/SCCS_data-for-course/Route-data.csv")

routes <- routes_raw

names(routes)
```
use inner_join (making sure we check the number or rows stays the same)

```{r inner_join}
# inner_join will 'guess' the variables by which to join the two datasets
dat_std <- inner_join(x = dat, y = routes)

# but safer practise is to explicitly state the join by variables 
dat_std <- inner_join(x = dat, y = routes, by = c("project", "route", "meanlong", "region"))

# number of rows before join
nrow(dat)
# and after the join:
nrow(dat_std)
```
Um, how come the number rows has increased after the join?

## Dangerous joins
Joins are a very important but very dangerous data wrangling operation! You must always choose your join type carefully. For instance, inner_join vs left_join vs full_join will all give the same result for some datasets, but not others.

So let’s do a bit more of a thorough check of the routes data:

```{r length unique}
nrow(routes)
## [1] 28
# how many different routes are there?
length(unique(routes$route))
## [1] 25
```
Oops. The routes data has duplicate entries. So let’s now check if duplicated routes have some matching data:

```{r duplicated}
### identify duplicated routes
idup <- duplicated(routes$route)

### extracted duplicated route names
dup_routes <- routes$route[idup]

### select rows where 'route' matches any of the route names listed in dup_routes (?'%in%')
filter(routes, route %in% dup_routes)
```
Luckily the duplicated routes have the same values for all variables (e.g., silk_area); if they didn’t we’d have to go back to the data provider and find out which ones were correct. But since they are the same, we can just remove the duplicates. This is easy with a dplyr function distinct(), which selects distinct entries:

```{r distinct}
routes2 <- distinct(routes)

nrow(routes2)
```
Our routes2 dataframe now consiss of 25 rows which is the same as the number of unique routes (length(unique(routes$route))). Great! Now try the join again, and do a few checks to make sure it worked as expected.

```{r}
dat_std <- inner_join(x = dat, y = routes2, by = c("project", "route", "meanlong", "region"))
### number of rows before join
nrow(dat)
## [1] 5313
### number of rows after join
nrow(dat_std)
## [1] 5313
sum(dat$segment_no == dat_std$segment_no)
## [1] 5313
# TRUE = 1 so for every time this is true you get a 1 so the sum should be the number of rows because they should all match and be true
```
## Add new variables with mutate
Once we have a matching silk_area value for each sample, it is easy to add a new variable that is standardised richness. 
To do this we use mutate which takes existing variables and calculates a new variable (or overwrites an existing one if we give it the same name). 
In addition to the standardised variables, we will also calculate the number of species per individual observed.

```{r mutate}
dat_std <-  
  mutate(dat_std,
         region = factor(region, levels = c("East", "West", "Southern Ocean")),
         richness = richness_raw / silk_area)
```
We’ve also made region a factor, which means we get to choose the order of the levels. This will be handy later.

Ok, let’s plot standardized richness so we can send a new graph to Prof Calanoid:

```{r ggplot}
ggplot(data = dat_std, 
       aes(x = latitude, y = richness)) +
  geom_point() + 
  geom_smooth() # trendline
```
The little message informs us that the spline is cubic regression spline (‘cs’) made with ‘gam’ (generalized additive model).

We can also fit a linear model, but a linear relationship is clearly not a good fit to these data:

```{r geom_smooth}
ggplot(dat_std, aes(x = latitude, y = richness, color = richness)) +
  geom_point() +
  geom_smooth(method = "lm") 
```
Important: this is good for visual checking, but doesn’t tell us much about how the model performed. To do that we need to go back to the basic R code for building that model. We won’t do this today, but these additional steps can be found in the full course full course.

http://www.seascapemodels.org/data/data-wrangling-spatial-course.html

We should also save the standardised data for use later:

```{r write_csv}
write_csv(x = dat_std, path = "SSCS_data/spatial-data/copepods_standardised.csv")
```
## Multipanel ggplots
ggplot2 isn’t limited to single panel plots. And that is just as well, because we have a sneaking suspicion that Prof Calanoid might have neglected to tell us something else about the data.

You might have noticed the CPR data covers both the East and West Coast of Australia, and the Southern Ocean. Well it would be fair to say that different ocean basins might have slightly different latitudinal patterns of richness.

So let’s investigate patterns by different oceans. You may have noticed that there is a region variable in the routes dataframe. Well that is joined into our standardized samples, so why don’t we start by just plotting the samples coloured by  region to check it out:

```{r ggplot group and color}
ggplot(dat_std, 
       aes(x = longitude, y = latitude, group = region, color = region)) +
  geom_point()
```
Looks like a good place to start. We could plot all the results, colouring by ocean basins like this:

```{r ggplot with trendlines}
ggplot(dat_std, 
       aes(x = latitude, y = richness, color = region, group = region)) +
  geom_point(size = 0.1) +
  geom_smooth() 
```
Notice that the use of group = region has meant that ggplot fits a smooth curve to each region too. Pretty handy, now we can see what looks like an interaction between latitude and East vs West. It also looks like the southern ocean pattern is a continuation of the West coast pattern.

By specifying size = 0.1 we reduce the size of the points to make the trend stand out.

These results are pretty exciting, but when we email this plot to Prof Calanoid she complains that the East and West data are too hard to tell apart.

Since the plot is a bit busy, it might help Prof Calanoid see these stunning results if we ‘facet’ by region.

```{r}
# Here's a fun trick: by wrapping the entire ggplot call and object assignment in parentheses, we can both save the plot as an object (cope_smooth) and view the plot without having to re-type 'cope_smooth'

(cope_smooth <- 
   ggplot(dat_std, aes(x = latitude, y = richness, group = region, color = region)) +
   geom_point(size = 0.1) +
   geom_smooth() +
   facet_grid(. ~ region) + #facet_grid adds the facets.  The . ~ region simply means add regions in columns versus nothing (.) in rows. If we replaced the . with another category, like project it would create a grid with rows and columns.
   theme_bw())

ggplot(dat_std, aes(x = latitude, y = richness, group = region, color = region)) +
   geom_point(size = 0.1) +
   geom_smooth() +
   facet_grid(route ~ region) + # replaced the . with another category to create a grid with rows and columns.
   theme_bw()
```

We wanted to compare the smooths across regions, so we’ve put regions all on one row this time. These plots are helpful because it allows for a quick and easy visual check for interactive effects. It is pretty clear now that we need to be cautious interpreting regional differences between the species richness * latitude relationship because there is no overlap in latitude between the Southern Ocean and the other two regions.

The only other change we made was to add the theme_bw layer, which, most noticeably, changes the background colour from gray to white.

```{r ggsave}
ggsave(plot = cope_smooth, filename = "figs/richness-latitude-smooth.jpeg",
       width = 16, height = 10, units = "cm", dpi = 300)
```
## Grouping and summarizing
All the plots and pretty smooths are nice, but seeing the hard numbers can be useful too. Prof Calanoid is interested to see how richness changes according to the different survey routes.

To do this we’re going to get into grouping with the group_by function followed by summarize to derive summary statistics for each group.

group_by adds a tag to the dataframe to indicate what variables to group the data by prior to calculating summary statistics (e.g., mean).

Let’s see the two functions in action.

```{r group and summarize}
### how many rows in data_std
nrow(dat_std)
### how many different routes in dat_std
n_distinct(dat_std$route)
### group dat_std by route
datg <- group_by(dat_std, route)
# or with a pipe
dat_std %>% group_by(., route)
```
Look at datg to convince yourself that the data themselves have not changed, but if you type datg into the console it will print out Groups: route [25], indicating the data is grouped by 25 routes.

What we want to do is put group_by together with a summarize:

```{r group and summarise}
(dat_std_sum <- 
   summarize(datg,
             mean_rich = mean(richness),
             sd_rich = sd(richness),
             n = n())) # the counts I think
```
With piping, The argument dat_std before the %>% just gets dropped in place of the .. Pipes are handy for chaining together multi-step operations on dataframes, like grouping, summarizing, and mutating. For most pipe-compatible functions, the . is not even needed, which makes our code even nicer to read.

Here we calculate the mean and standard deviation of richness for each route, as well as the number of rows (samples) n() for each route:

```{r piping}
dat_std_sum <- 
  dat_std %>% 
  group_by(route) %>%
  summarize(mean_rich  = mean(richness),
            sd_rich = sd(richness),
            n = n())
```
Here we combine group_by, mutate, summarize, and ggplot using piping to plot the mean (+/- standard error) species richness by route, where routes are sorted by latitude on the x-axis

```{r group_by, mutate, summarize, and ggplot}
dat_std %>% 
  group_by(route, meanlat, meanlong) %>%
  summarize(mean_rich  = mean(richness),
            sd_rich = sd(richness),
            n = n(), 
            se_rich = sd_rich / sqrt(n)) %>%
  ungroup() %>%
  mutate(route = fct_reorder(.f = route, .x = meanlat, .fun = max)) %>%
  ggplot(aes(x = route, y = mean_rich)) +
  geom_linerange(aes(ymin = mean_rich - se_rich, ymax = mean_rich + se_rich)) +
  geom_point()
```
## Introduction to mapping and spatial analysis in R
In the first section we tidied up Prof Calanoid’s and conducted some exploratory (mostly visual) analyses of the relationship between latitude and species richness. But the job isn’t done yet.

Prof Calanoid’s actual hypothesis was about sea surface temperature and she also wanted to see some maps and for us to create an interactive map to share with funders.

In this section of the course we will look at how to combine our copepod data with spatial layers for temperature and then create nice static and interactive maps.

#A simple map of sample sites
Let’s work from the dat_std:

```{r overlay on worldmap}
names(dat_std)


```

